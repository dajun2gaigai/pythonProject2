import argparse
import torch
import torchvision.datasets
from torch import nn
from d2l import torch as d2l
from torch.nn import functional as F

'''
PyTorchçš„è®¡ç®—å°†å°è¯•ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
gpuè®¾å¤‡åªä»£è¡¨ä¸€ä¸ªå¡å’Œç›¸åº”çš„æ˜¾å­˜ã€‚å¦‚æœæœ‰å¤šä¸ªGPUï¼Œæˆ‘ä»¬ä½¿ç”¨torch.cuda.device(f'cuda:{i}')æ¥è¡¨ç¤ºç¬¬ ğ‘– å—GPUï¼ˆ ğ‘– ä»0å¼€å§‹ï¼‰ã€‚
å¦å¤–ï¼Œgpu:0å’Œgpuæ˜¯ç­‰ä»·çš„
æ‰“å°å¼ é‡æˆ–å°†å¼ é‡è½¬æ¢ä¸ºNumPyæ ¼å¼æ—¶ã€‚å¦‚æœæ•°æ®ä¸åœ¨å†…å­˜ä¸­ï¼Œæ¡†æ¶ä¼šé¦–å…ˆå°†å…¶å¤åˆ¶åˆ°å†…å­˜ä¸­ï¼Œè¿™ä¼šå¯¼è‡´é¢å¤–çš„ä¼ è¾“å¼€é”€
'''
torch.device('cpu')
torch.device('cuda')
# torch.cuda.device(f'cuda:{i}')
torch.cuda.device_count()
print(torch.cuda.device_count())

def try_gpu(i=0):
    if torch.cuda.device_count() >= i+1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
def try_all_gpus():
    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else torch.device('cpu')

#é»˜è®¤åœ¨cpu
x = torch.tensor([1,2,3])
x.device
x = torch.tensor([1,2,3], device=try_gpu())
x = torch.tensor([1,2,3],device=torch.device('cuda:0'))
print(x)

#ä¸åœ¨åŒä¸€ä¸ªGPUä¸Šçš„æ•°æ®ä¸èƒ½æ‰§è¡Œç›¸åŠ æ“ä½œï¼Œéœ€è¦å¤åˆ¶åˆ°ä¸€èµ·
X = torch.ones((2,3), device=torch.device('cuda:0'))
Y = torch.ones((2,3), device=torch.device('cpu'))
Z = X + Y.clone().to(X.device)
Z = X + Y.cuda(0)
print(Z)

#ç¥ç»ç½‘ç»œå‚æ•°æŸ¥çœ‹
net = nn.Sequential(nn.Linear(3,10))
net = net.to(try_gpu())
net(X)
print(net[0])
print(net[0].weight.data.device)

class Residual(nn.Module):
    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
        super(Residual,self).__init__()
        self.conv1 = nn.Conv2d(input_channels,num_channels,kernel_size=3,stride=strides,padding=1)
        self.conv2 = nn.Conv2d(num_channels,num_channels,kernel_size=3,padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels,num_channels,kernel_size=1,stride=strides)
        else:
            self.conv3 = None

        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self,X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)

def resnet18(num_classes, in_channels=1):
    """ç¨åŠ ä¿®æ”¹çš„ResNet-18æ¨¡å‹ã€‚"""
    # ResNetä½¿ç”¨4ä¸ªæ®‹å·®å—,ç¬¬ä¸€ä¸ªå—è¾“å…¥è¾“å‡ºé€šé“ä¸å˜ï¼Œä»¥åæ¯ä¸ªå—è¾“å‡ºé€šé“å‡åŠï¼Œé•¿å®½å‡åŠ
    # å¯¹ç¬¬ä¸€ä¸ªæ®‹å·®å—åšç‰¹æ®Šå¤„ç†,ä¸æ”¹å˜é•¿å®½åŠé€šé“
    def resnet_block(in_channels, out_channels, num_residuals,
                     first_block=False):
        blk = []
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.append(
                    Residual(in_channels, out_channels, use_1x1conv=True,
                                 strides=2))
            else:
                blk.append(Residual(out_channels, out_channels))
        return nn.Sequential(*blk)

    # è¯¥æ¨¡å‹ä½¿ç”¨äº†æ›´å°çš„å·ç§¯æ ¸ã€æ­¥é•¿å’Œå¡«å……ï¼Œä¸”åˆ é™¤äº†æœ€å¤§æ± åŒ–å±‚ã€‚
    net = nn.Sequential(
        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(64), nn.ReLU())
    net.add_module("resnet_block1", resnet_block(64, 64, 2, first_block=True))
    net.add_module("resnet_block2", resnet_block(64, 128, 2))
    net.add_module("resnet_block3", resnet_block(128, 256, 2))
    net.add_module("resnet_block4", resnet_block(256, 512, 2))
    net.add_module("global_avg_pool", nn.AdaptiveAvgPool2d((1, 1)))
    net.add_module("fc",
                   nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))
    return net
def try_all_gpus():
    """Return all available GPUs, or [cpu(),] if no GPU exists."""
    devices = [
        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

# Defined in file: ./chapter_deep-learning-computation/use-gpu.md
def try_gpu(i=0):
    """Return gpu(i) if exists, otherwise return cpu()."""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

net = resnet18(10)
devices = try_all_gpus()

def train(net,num_gpus,batch_size,lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    '''
    1 åˆå§‹åŒ–ç½‘ç»œå‚æ•°
    2 å¤åˆ¶ç½‘è·¯åˆ°æ¯ä¸ªGPU
    3 å°†mini-batchæ•°æ®åˆ†å‰²ååˆ†é…ç»™æ¯ä¸ªGPU
    4 å„ä¸ªGPUè®¡ç®—æŸå¤±å’Œæ¢¯åº¦
    5 èšåˆæ¢¯åº¦ï¼Œå¹¿æ’­ï¼Œæ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°'''
    def init_weights(m):
        if type(m) in [nn.Linear,nn.Conv2d]:
            nn.init.normal_(m.weight,std=0.01)
    net.apply(init_weights)

    #åœ¨å¤šGPUä¸Šè®¾ç½®æ¨¡å‹
    net = nn.DataParallel(net,device_ids=devices)
    optimizer = torch.optim.SGD(net.parameters(),lr)
    loss = nn.CrossEntropyLoss()
    timer, num_epochs = d2l.Timer(),10
    animator = d2l.Animator('epoch','test acc', xlim=[1,num_epochs])
    for epoch in range(num_epochs):
        net.train()
        timer.start()
        for X,y in train_iter:
            optimizer.zero_grad()
            X,y = X.to(devices[0]), y.to(devices[0])
            l = loss(net(X),y)
            l.backward()
            optimizer.step()
        timer.stop()
        animator.add(epoch+1,(d2l.evaluate_accuracy_gpu(net,test_iter),))
    print(f'test acc: {animator.Y[0][-1]:.2f},{timer.avg():.1f} sec/epoch '
          f'on {str(devices)}')
train(net,num_gpus=1,batch_size=1,lr=0.1)